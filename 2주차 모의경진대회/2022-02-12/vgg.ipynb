{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7d3709d",
   "metadata": {},
   "source": [
    "# [모의 캐글-의료] 흉부 CT 코로나 감염 여부 분류\n",
    "- 이미지 binary 분류 과제\n",
    "- 담당: 이녕민M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a9c264",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5111a505",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !apt-get update && apt-get install -y python3-opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bad4cfc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install sklearn\n",
    "# !pip install tqdm\n",
    "# !pip install torchvision\n",
    "# !pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d98533f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch, copy, cv2, sys, random\n",
    "# from datetime import datetime, timezone, timedelta\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd257f73",
   "metadata": {},
   "source": [
    "## Set Arguments & hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9408422e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시드(seed) 설정\n",
    "\n",
    "RANDOM_SEED = 2022\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2084b811",
   "metadata": {},
   "source": [
    "# 데이터 디렉토리 구조\n",
    "\n",
    "data/  \n",
    "  \\_train/  \n",
    "    \\_0.png  \n",
    "    \\_1.png  \n",
    "    \\_...  \n",
    "  \\_test/  \n",
    "    \\_0.png  \n",
    "    \\_1.png  \n",
    "    \\_...  \n",
    "  \\_train.csv  \n",
    "  \\_sample_submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86284fed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameters\n",
    "\n",
    "### 데이터 디렉토리 설정 ###\n",
    "DATA_DIR= 'data'\n",
    "NUM_CLS = 2\n",
    "\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.0005\n",
    "EARLY_STOPPING_PATIENCE = 10\n",
    "\n",
    "# vggnet, resnet = 224\n",
    "# basecnn = 128\n",
    "INPUT_SHAPE = 224\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# DEVICE = torch.device('cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23d9404b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fccdccb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import cv2\n",
    "# import glob\n",
    "# from PIL import Image\n",
    "# import PIL.ImageOps    \n",
    "\n",
    "# #다음 변수를 수정하여 새로 만들 이미지 갯수를 정합니다.\n",
    "# num_augmented_images = 400\n",
    "\n",
    "# file_path = 'data/train/'\n",
    "# file_names = os.listdir(file_path)\n",
    "# total_origin_image_num = len(file_names)\n",
    "# augment_cnt = 1\n",
    "\n",
    "# for i in range(1, num_augmented_images):\n",
    "#     change_picture_index = random.randrange(1, total_origin_image_num-1)\n",
    "#     print(change_picture_index)\n",
    "#     print(file_names[change_picture_index])\n",
    "#     file_name = file_names[change_picture_index]\n",
    "    \n",
    "#     origin_image_path = 'data/train/' + file_name\n",
    "#     print(origin_image_path)\n",
    "#     image = Image.open(origin_image_path)\n",
    "#     random_augment = random.randrange(1,4)\n",
    "    \n",
    "#     if(random_augment == 1):\n",
    "#         #이미지 좌우 반전\n",
    "#         print(\"invert\")\n",
    "#         inverted_image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "#         inverted_image.save(file_path + 'inverted_' + str(augment_cnt) + '.png')\n",
    "        \n",
    "#     elif(random_augment == 2):\n",
    "#         #이미지 기울이기\n",
    "#         print(\"rotate\")\n",
    "#         rotated_image = image.rotate(random.randrange(-20, 20))\n",
    "#         rotated_image.save(file_path + 'rotated_' + str(augment_cnt) + '.png')\n",
    "        \n",
    "#     elif(random_augment == 3):\n",
    "#         #노이즈 추가하기\n",
    "#         img = cv2.imread(origin_image_path)\n",
    "#         print(\"noise\")\n",
    "#         row,col,ch= img.shape\n",
    "#         mean = 0\n",
    "#         var = 0.1\n",
    "#         sigma = var**0.5\n",
    "#         gauss = np.random.normal(mean,sigma,(row,col,ch))\n",
    "#         gauss = gauss.reshape(row,col,ch)\n",
    "#         noisy_array = img + gauss\n",
    "#         noisy_image = Image.fromarray(np.uint8(noisy_array)).convert('RGB')\n",
    "#         noisy_image.save(file_path + 'noiseAdded_' + str(augment_cnt) + '.png')\n",
    "        \n",
    "#     augment_cnt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e339217d",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6820c75",
   "metadata": {},
   "source": [
    "#### Train & Validation Set loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42bad363",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir, mode, input_shape):\n",
    "        self.data_dir = data_dir\n",
    "        self.mode = mode\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "        # Loading dataset\n",
    "        self.db = self.data_loader()\n",
    "        \n",
    "        # Dataset split\n",
    "        if self.mode == 'train':\n",
    "            self.db = self.db[:int(len(self.db) * 0.9)]\n",
    "        elif self.mode == 'val':\n",
    "            self.db = self.db[int(len(self.db) * 0.9):]\n",
    "            self.db.reset_index(inplace=True)\n",
    "        else:\n",
    "            print(f'!!! Invalid split {self.mode}... !!!')\n",
    "            \n",
    "        # Transform function\n",
    "        self.transform = transforms.Compose([transforms.Resize(self.input_shape),\n",
    "                                             transforms.ToTensor(),\n",
    "                                             transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "    def data_loader(self):\n",
    "        print('Loading ' + self.mode + ' dataset..')\n",
    "        if not os.path.isdir(self.data_dir):\n",
    "            print(f'!!! Cannot find {self.data_dir}... !!!')\n",
    "            sys.exit()\n",
    "        \n",
    "        # (COVID : 1, No : 0)\n",
    "        db = pd.read_csv(os.path.join(self.data_dir, 'train.csv'))\n",
    "        \n",
    "        return db\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.db)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = copy.deepcopy(self.db.loc[index])\n",
    "\n",
    "        # Loading image\n",
    "        cvimg = cv2.imread(os.path.join(self.data_dir,'train',data['file_name']), cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION)\n",
    "        if not isinstance(cvimg, np.ndarray):\n",
    "            raise IOError(\"Fail to read %s\" % data['file_name'])\n",
    "\n",
    "        # Preprocessing images\n",
    "        trans_image = self.transform(Image.fromarray(cvimg))\n",
    "\n",
    "        return trans_image, data['COVID']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b3228f",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d463765",
   "metadata": {},
   "source": [
    "## VGGNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "727b5fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG_types = {\n",
    "    'VGG11' : [64, 'M', 128, 'M', 256, 256, 'M', 512,512, 'M',512,512,'M'],\n",
    "    'VGG13' : [64,64, 'M', 128, 128, 'M', 256, 256, 'M', 512,512, 'M', 512,512,'M'],\n",
    "    'VGG16' : [64,64, 'M', 128, 128, 'M', 256, 256,256, 'M', 512,512,512, 'M',512,512,512,'M'],\n",
    "    'VGG19' : [64,64, 'M', 128, 128, 'M', 256, 256,256,256, 'M', 512,512,512,512, 'M',512,512,512,512,'M']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ca7f71b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "VGGnet(\n",
      "  (conv_layers): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): ReLU()\n",
      "    (14): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (17): ReLU()\n",
      "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (20): ReLU()\n",
      "    (21): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (24): ReLU()\n",
      "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (27): ReLU()\n",
      "    (28): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fcs): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define VGGnet class\n",
    "class VGGnet(nn.Module):\n",
    "    def __init__(self, model, in_channels=3, num_classes=NUM_CLS, init_weights=True):\n",
    "        super(VGGnet,self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        # create conv_layers corresponding to VGG type\n",
    "        self.conv_layers = self.create_conv_laters(VGG_types[model])\n",
    "\n",
    "        self.fcs = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        # weight initialization\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(-1, 512 * 7 * 7)\n",
    "        x = self.fcs(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "    # defint weight initialization function\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    # define a function to create conv layer taken the key of VGG_type dict \n",
    "    def create_conv_laters(self, architecture):\n",
    "        layers = []\n",
    "        in_channels = self.in_channels # 3\n",
    "\n",
    "        for x in architecture:\n",
    "            if type(x) == int: # int means conv layer\n",
    "                out_channels = x\n",
    "\n",
    "                layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                     kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU()]\n",
    "                in_channels = x\n",
    "            elif x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))]\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "# define device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# creat VGGnet object\n",
    "model = VGGnet('VGG11', in_channels=3, num_classes=NUM_CLS, init_weights=True).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f3b24a",
   "metadata": {},
   "source": [
    "## torchvision.models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a6da43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision import models\n",
    "\n",
    "# model = models.resnet18(pretrained=False).to(DEVICE)\n",
    "# model = models.vgg16(pretrained=False).to(DEVICE)\n",
    "\n",
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0141b3",
   "metadata": {},
   "source": [
    "## base CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "697b27c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn.functional as F\n",
    "\n",
    "# class custom_CNN(nn.Module):\n",
    "#     def __init__(self, num_classes):\n",
    "#         super(custom_CNN, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=5)\n",
    "#         self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "#         self.conv2 = nn.Conv2d(in_channels=8, out_channels=25, kernel_size=5)\n",
    "        \n",
    "#         self.fc1 = nn.Linear(in_features=25*29*29, out_features=128)\n",
    "#         self.fc2 = nn.Linear(in_features=128, out_features=num_classes)\n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(F.relu(self.conv1(x))) # (32, 3, 128, 128) -> (32, 8, 62, 62)\n",
    "#         x = self.pool(F.relu(self.conv2(x))) # (32, 8, 62, 62) -> (32, 25, 29, 29)\n",
    "        \n",
    "#         x = torch.flatten(x,1)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "        \n",
    "#         output = self.softmax(x)\n",
    "        \n",
    "#         return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9e7ec5",
   "metadata": {},
   "source": [
    "# summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2eef65aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummary in /opt/conda/lib/python3.8/site-packages (1.5.1)\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "       BatchNorm2d-2         [-1, 64, 224, 224]             128\n",
      "              ReLU-3         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-4         [-1, 64, 112, 112]               0\n",
      "            Conv2d-5        [-1, 128, 112, 112]          73,856\n",
      "       BatchNorm2d-6        [-1, 128, 112, 112]             256\n",
      "              ReLU-7        [-1, 128, 112, 112]               0\n",
      "         MaxPool2d-8          [-1, 128, 56, 56]               0\n",
      "            Conv2d-9          [-1, 256, 56, 56]         295,168\n",
      "      BatchNorm2d-10          [-1, 256, 56, 56]             512\n",
      "             ReLU-11          [-1, 256, 56, 56]               0\n",
      "           Conv2d-12          [-1, 256, 56, 56]         590,080\n",
      "      BatchNorm2d-13          [-1, 256, 56, 56]             512\n",
      "             ReLU-14          [-1, 256, 56, 56]               0\n",
      "        MaxPool2d-15          [-1, 256, 28, 28]               0\n",
      "           Conv2d-16          [-1, 512, 28, 28]       1,180,160\n",
      "      BatchNorm2d-17          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-18          [-1, 512, 28, 28]               0\n",
      "           Conv2d-19          [-1, 512, 28, 28]       2,359,808\n",
      "      BatchNorm2d-20          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-21          [-1, 512, 28, 28]               0\n",
      "        MaxPool2d-22          [-1, 512, 14, 14]               0\n",
      "           Conv2d-23          [-1, 512, 14, 14]       2,359,808\n",
      "      BatchNorm2d-24          [-1, 512, 14, 14]           1,024\n",
      "             ReLU-25          [-1, 512, 14, 14]               0\n",
      "           Conv2d-26          [-1, 512, 14, 14]       2,359,808\n",
      "      BatchNorm2d-27          [-1, 512, 14, 14]           1,024\n",
      "             ReLU-28          [-1, 512, 14, 14]               0\n",
      "        MaxPool2d-29            [-1, 512, 7, 7]               0\n",
      "           Linear-30                 [-1, 4096]     102,764,544\n",
      "             ReLU-31                 [-1, 4096]               0\n",
      "          Dropout-32                 [-1, 4096]               0\n",
      "           Linear-33                 [-1, 4096]      16,781,312\n",
      "             ReLU-34                 [-1, 4096]               0\n",
      "          Dropout-35                 [-1, 4096]               0\n",
      "           Linear-36                    [-1, 2]           8,194\n",
      "          Softmax-37                    [-1, 2]               0\n",
      "================================================================\n",
      "Total params: 128,780,034\n",
      "Trainable params: 128,780,034\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 181.83\n",
      "Params size (MB): 491.26\n",
      "Estimated Total Size (MB): 673.66\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary\n",
    "import torchsummary\n",
    "\n",
    "torchsummary.summary(model, input_size = (3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcae531",
   "metadata": {},
   "source": [
    "## Utils\n",
    "### EarlyStopper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95cb6747",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossEarlyStopper():\n",
    "    \"\"\"Early stopper\n",
    "    \n",
    "    Attributes:\n",
    "        patience (int): loss가 줄어들지 않아도 학습할 epoch 수\n",
    "        patience_counter (int): loss 가 줄어들지 않을 때 마다 1씩 증가, 감소 시 0으로 리셋\n",
    "        min_loss (float): 최소 loss\n",
    "        stop (bool): True 일 때 학습 중단\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patience: int)-> None:\n",
    "        self.patience = patience\n",
    "\n",
    "        self.patience_counter = 0\n",
    "        self.min_loss = np.Inf\n",
    "        self.stop = False\n",
    "        self.save_model = False\n",
    "\n",
    "    def check_early_stopping(self, loss: float)-> None:\n",
    "        \"\"\"Early stopping 여부 판단\"\"\"  \n",
    "\n",
    "        if self.min_loss == np.Inf:\n",
    "            self.min_loss = loss\n",
    "            return None\n",
    "\n",
    "        elif loss > self.min_loss:\n",
    "            self.patience_counter += 1\n",
    "            msg = f\"Early stopping counter {self.patience_counter}/{self.patience}\"\n",
    "\n",
    "            if self.patience_counter == self.patience:\n",
    "                self.stop = True\n",
    "                \n",
    "        elif loss <= self.min_loss:\n",
    "            self.patience_counter = 0\n",
    "            self.save_model = True\n",
    "            msg = f\"Validation loss decreased {self.min_loss} -> {loss}\"\n",
    "            self.min_loss = loss\n",
    "        \n",
    "        print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0587f566",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "418722a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    \"\"\" epoch에 대한 학습 및 검증 절차 정의\"\"\"\n",
    "    \n",
    "    def __init__(self, loss_fn, model, device, metric_fn, optimizer=None, scheduler=None):\n",
    "        \"\"\" 초기화\n",
    "        \"\"\"\n",
    "        self.loss_fn = loss_fn\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.metric_fn = metric_fn\n",
    "\n",
    "    def train_epoch(self, dataloader, epoch_index):\n",
    "        \"\"\" 한 epoch에서 수행되는 학습 절차\"\"\"\n",
    "        \n",
    "        self.model.train()\n",
    "        train_total_loss = 0\n",
    "        target_lst = []\n",
    "        pred_lst = []\n",
    "        prob_lst = []\n",
    "\n",
    "        for batch_index, (img, label) in enumerate(dataloader):\n",
    "            img = img.to(self.device)\n",
    "            label = label.to(self.device).float()\n",
    "\n",
    "            \n",
    "            pred = self.model(img)\n",
    "            \n",
    "            # pred = torch.softmax(pred)\n",
    "            # pred = torch.round(pred)\n",
    "            \n",
    "            loss = self.loss_fn(pred[:,1], label)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            train_total_loss += loss.item()\n",
    "            prob_lst.extend(pred[:, 1].cpu().tolist())\n",
    "            target_lst.extend(label.cpu().tolist())\n",
    "            pred_lst.extend(pred.argmax(dim=1).cpu().tolist())\n",
    "        self.train_mean_loss = train_total_loss / batch_index\n",
    "        self.train_score, f1 = self.metric_fn(y_pred=pred_lst, y_answer=target_lst)\n",
    "        msg = f'Epoch {epoch_index}, Train loss: {self.train_mean_loss}, Acc: {self.train_score}, F1-Macro: {f1}'\n",
    "        print(msg)\n",
    "\n",
    "    def validate_epoch(self, dataloader, epoch_index):\n",
    "        \"\"\" 한 epoch에서 수행되는 검증 절차\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        val_total_loss = 0\n",
    "        target_lst = []\n",
    "        pred_lst = []\n",
    "        prob_lst = []\n",
    "\n",
    "        for batch_index, (img, label) in enumerate(dataloader):\n",
    "            img = img.to(self.device)\n",
    "            label = label.to(self.device).float()\n",
    "            \n",
    "            pred = self.model(img)\n",
    "            \n",
    "            # pred = torch.softmax(pred)\n",
    "            # pred = torch.round(pred)\n",
    "            \n",
    "            loss = self.loss_fn(pred[:,1], label)\n",
    "            val_total_loss += loss.item()\n",
    "            prob_lst.extend(pred[:, 1].cpu().tolist())\n",
    "            target_lst.extend(label.cpu().tolist())\n",
    "            pred_lst.extend(pred.argmax(dim=1).cpu().tolist())\n",
    "        self.val_mean_loss = val_total_loss / batch_index\n",
    "        self.validation_score, f1 = self.metric_fn(y_pred=pred_lst, y_answer=target_lst)\n",
    "        msg = f'Epoch {epoch_index}, Val loss: {self.val_mean_loss}, Acc: {self.validation_score}, F1-Macro: {f1}'\n",
    "        print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0501aa",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75ae50d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def get_metric_fn(y_pred, y_answer):\n",
    "    \"\"\" 성능을 반환하는 함수\"\"\"\n",
    "    \n",
    "    assert len(y_pred) == len(y_answer), 'The size of prediction and answer are not same.'\n",
    "    accuracy = accuracy_score(y_answer, y_pred)\n",
    "    f1 = f1_score(y_answer, y_pred, average='macro')\n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc1efd8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train\n",
    "### 학습을 위한 객체 선언"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77ea8e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Load Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e67e442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataset..\n",
      "Loading val dataset..\n",
      "Train set samples: 581 Val set samples: 65\n"
     ]
    }
   ],
   "source": [
    "# Load dataset & dataloader\n",
    "train_dataset = CustomDataset(data_dir=DATA_DIR, mode='train', input_shape=INPUT_SHAPE)\n",
    "# train_dataset2 = CustomDataset(data_dir=DATA_DIR, mode='train', input_shape=INPUT_SHAPE)\n",
    "validation_dataset = CustomDataset(data_dir=DATA_DIR, mode='val', input_shape=INPUT_SHAPE)\n",
    "\n",
    "# train_dataset = torch.utils.data.ConcatDataset([train_dataset, train_dataset2])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "print('Train set samples:',len(train_dataset),  'Val set samples:', len(validation_dataset))\n",
    "\n",
    "#581 - 65"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8c761b",
   "metadata": {},
   "source": [
    "#### Load model and other utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90319a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "# model = custom_CNN(NUM_CLS).to(DEVICE)\n",
    "\n",
    "\n",
    "# # Save Initial Model\n",
    "# torch.save(model.state_dict(), 'initial.pt')\n",
    "\n",
    "# Set optimizer, scheduler, loss function, metric function\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler =  optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e5, max_lr=0.0001, epochs=EPOCHS, steps_per_epoch=len(train_dataloader))\n",
    "loss_fn = nn.BCELoss()\n",
    "metric_fn = get_metric_fn\n",
    "\n",
    "# Set trainer\n",
    "trainer = Trainer(loss_fn, model, DEVICE, metric_fn, optimizer, scheduler)\n",
    "\n",
    "# Set earlystopper\n",
    "early_stopper = LossEarlyStopper(patience=EARLY_STOPPING_PATIENCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c368b973",
   "metadata": {},
   "source": [
    "### epoch 단위 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5042c15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjw_cho\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/jw_cho/test2/runs/n0kj6ywv\" target=\"_blank\">vgg13</a></strong> to <a href=\"https://wandb.ai/jw_cho/test2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train loss: 0.7909529440932803, Acc: 0.5163511187607573, F1-Macro: 0.5133002429608431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3% 1/30 [00:50<24:29, 50.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Val loss: 0.9520248770713806, Acc: 0.49230769230769234, F1-Macro: 0.32989690721649484\n",
      "Epoch 1, Train loss: 0.6777223216162788, Acc: 0.6833046471600689, F1-Macro: 0.6806471656629385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7% 2/30 [01:41<23:38, 50.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Val loss: 3.120003581047058, Acc: 0.49230769230769234, F1-Macro: 0.32989690721649484\n",
      "Early stopping counter 1/10\n",
      "Epoch 2, Train loss: 1.2434372355540593, Acc: 0.6506024096385542, F1-Macro: 0.6483983961603244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10% 3/30 [02:29<22:30, 50.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Val loss: 4.051737189292908, Acc: 0.6615384615384615, F1-Macro: 0.6595238095238096\n",
      "Early stopping counter 2/10\n",
      "Epoch 3, Train loss: 1.0475210679901972, Acc: 0.7022375215146299, F1-Macro: 0.7014415094115447\n",
      "Epoch 3, Val loss: 0.5575955111999065, Acc: 0.7230769230769231, F1-Macro: 0.7198275862068966\n",
      "Validation loss decreased 0.9520248770713806 -> 0.5575955111999065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13% 4/30 [03:21<21:56, 50.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train loss: 0.9416714389291074, Acc: 0.6919104991394148, F1-Macro: 0.6917315074177819\n",
      "Epoch 4, Val loss: 3.295167326927185, Acc: 0.5230769230769231, F1-Macro: 0.39398496240601505\n",
      "Early stopping counter 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17% 5/30 [04:13<21:14, 50.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train loss: 0.6874047459827529, Acc: 0.7693631669535284, F1-Macro: 0.7675722474325293\n",
      "Epoch 5, Val loss: 0.7127618193626404, Acc: 0.6307692307692307, F1-Macro: 0.5962732919254659\n",
      "Early stopping counter 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20% 6/30 [05:05<20:30, 51.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train loss: 0.3537810875309838, Acc: 0.8571428571428571, F1-Macro: 0.8560769531518142\n",
      "Epoch 6, Val loss: 0.5797915467992425, Acc: 0.7230769230769231, F1-Macro: 0.7230113636363636\n",
      "Early stopping counter 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23% 7/30 [06:07<20:49, 54.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train loss: 0.44431666243407464, Acc: 0.8519793459552496, F1-Macro: 0.8516096459966738\n",
      "Epoch 7, Val loss: 0.8093098104000092, Acc: 0.6615384615384615, F1-Macro: 0.6549227799227799\n",
      "Early stopping counter 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27% 8/30 [07:00<19:48, 54.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train loss: 0.47980647368563545, Acc: 0.8433734939759037, F1-Macro: 0.8421087210361438\n",
      "Epoch 8, Val loss: 0.5977616086602211, Acc: 0.6615384615384615, F1-Macro: 0.6575670498084292\n",
      "Early stopping counter 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30% 9/30 [07:54<18:57, 54.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Train loss: 0.3894614655938413, Acc: 0.8657487091222031, F1-Macro: 0.8652020274129069\n",
      "Epoch 9, Val loss: 0.6013123542070389, Acc: 0.7230769230769231, F1-Macro: 0.7075\n",
      "Early stopping counter 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33% 10/30 [08:47<17:56, 53.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train loss: 0.20345668701661956, Acc: 0.9259896729776248, F1-Macro: 0.9256008838673365\n",
      "Epoch 10, Val loss: 2.0046077370643616, Acc: 0.7230769230769231, F1-Macro: 0.7149122807017543\n",
      "Early stopping counter 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37% 11/30 [09:40<16:55, 53.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Train loss: 0.09955885789046685, Acc: 0.9672977624784854, F1-Macro: 0.9670914755026905\n",
      "Epoch 11, Val loss: 0.7252578288316727, Acc: 0.8, F1-Macro: 0.799239724400095\n",
      "Early stopping counter 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40% 12/30 [10:33<16:01, 53.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Train loss: 0.06543885792295139, Acc: 0.9793459552495697, F1-Macro: 0.9792524699440542\n",
      "Epoch 12, Val loss: 0.7639782056212425, Acc: 0.7692307692307693, F1-Macro: 0.7610879686351384\n",
      "Early stopping counter 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43% 13/30 [11:27<15:07, 53.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Train loss: 0.10664602006889051, Acc: 0.9500860585197934, F1-Macro: 0.9497136342337664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43% 13/30 [12:17<16:04, 56.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Val loss: 2.1796208173036575, Acc: 0.7384615384615385, F1-Macro: 0.7257383966244726\n",
      "Early stopping counter 10/10\n",
      "Early stopped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 65818... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>▅▅█▇▆▅▃▃▃▃▂▁▁▁</td></tr><tr><td>train_score</td><td>▁▄▃▄▄▅▆▆▆▆▇███</td></tr><tr><td>val_loss</td><td>▂▆█▁▆▁▁▂▁▁▄▁▁▄</td></tr><tr><td>val_score</td><td>▁▁▅▆▂▄▆▅▅▆▆█▇▇</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>0.10665</td></tr><tr><td>train_score</td><td>0.95009</td></tr><tr><td>val_loss</td><td>2.17962</td></tr><tr><td>val_score</td><td>0.73846</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">vgg13</strong>: <a href=\"https://wandb.ai/jw_cho/test2/runs/n0kj6ywv\" target=\"_blank\">https://wandb.ai/jw_cho/test2/runs/n0kj6ywv</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220212_133240-n0kj6ywv/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "# wandb.login(relogin = True)\n",
    "\n",
    "wandb.init(project = 'test2', name = 'vgg11')\n",
    "\n",
    "config = wandb.config\n",
    "config.learning_rate = LEARNING_RATE\n",
    "\n",
    "for epoch_index in tqdm(range(EPOCHS)):\n",
    "\n",
    "    trainer.train_epoch(train_dataloader, epoch_index)\n",
    "    trainer.validate_epoch(validation_dataloader, epoch_index)\n",
    "\n",
    "    # early_stopping check\n",
    "    early_stopper.check_early_stopping(loss=trainer.val_mean_loss)\n",
    "    \n",
    "    \n",
    "    wandb.log({\"train_loss\": trainer.train_mean_loss,\n",
    "             \"train_score\": trainer.train_score,\n",
    "             \"val_loss\": trainer.val_mean_loss,\n",
    "             \"val_score\": trainer.validation_score})\n",
    "\n",
    "    if early_stopper.stop:\n",
    "        print('Early stopped')\n",
    "        break\n",
    "\n",
    "    if early_stopper.save_model:\n",
    "        check_point = {\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict()\n",
    "        }\n",
    "        torch.save(check_point, 'best.pt')\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012125d5",
   "metadata": {},
   "source": [
    "## Inference\n",
    "### 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f62d41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINED_MODEL_PATH = 'best.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a523c2f0",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e08c99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, data_dir, input_shape):\n",
    "        self.data_dir = data_dir\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "        # Loading dataset\n",
    "        self.db = self.data_loader()\n",
    "        \n",
    "        # Transform function\n",
    "        self.transform = transforms.Compose([transforms.Resize(self.input_shape),\n",
    "                                             transforms.ToTensor(),\n",
    "                                             transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "    def data_loader(self):\n",
    "        print('Loading test dataset..')\n",
    "        if not os.path.isdir(self.data_dir):\n",
    "            print(f'!!! Cannot find {self.data_dir}... !!!')\n",
    "            sys.exit()\n",
    "        \n",
    "        db = pd.read_csv(os.path.join(self.data_dir, 'sample_submission.csv'))\n",
    "        return db\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.db)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = copy.deepcopy(self.db.loc[index])\n",
    "        \n",
    "        # Loading image\n",
    "        cvimg = cv2.imread(os.path.join(self.data_dir,'test',data['file_name']), cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION)\n",
    "        if not isinstance(cvimg, np.ndarray):\n",
    "            raise IOError(\"Fail to read %s\" % data['file_name'])\n",
    "\n",
    "        # Preprocessing images\n",
    "        trans_image = self.transform(Image.fromarray(cvimg))\n",
    "\n",
    "        return trans_image, data['file_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f166f20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test dataset..\n"
     ]
    }
   ],
   "source": [
    "# Load dataset & dataloader\n",
    "test_dataset = TestDataset(data_dir=DATA_DIR, input_shape=INPUT_SHAPE)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde480c0",
   "metadata": {},
   "source": [
    "### 추론 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "989ea822",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:02,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.9873e-01, 1.2660e-03],\n",
      "        [9.9977e-01, 2.3340e-04],\n",
      "        [9.9596e-01, 4.0385e-03],\n",
      "        [3.6632e-01, 6.3368e-01],\n",
      "        [9.9972e-01, 2.7624e-04],\n",
      "        [5.0853e-02, 9.4915e-01],\n",
      "        [7.2745e-01, 2.7255e-01],\n",
      "        [4.5852e-01, 5.4148e-01],\n",
      "        [8.9140e-01, 1.0860e-01],\n",
      "        [3.0813e-04, 9.9969e-01],\n",
      "        [9.9559e-01, 4.4139e-03],\n",
      "        [9.7065e-01, 2.9345e-02],\n",
      "        [9.9691e-01, 3.0869e-03],\n",
      "        [9.9826e-01, 1.7440e-03],\n",
      "        [9.7067e-01, 2.9333e-02],\n",
      "        [9.9173e-01, 8.2699e-03],\n",
      "        [9.9288e-01, 7.1180e-03],\n",
      "        [9.9999e-01, 1.3135e-05],\n",
      "        [9.3544e-01, 6.4565e-02],\n",
      "        [9.9869e-01, 1.3132e-03],\n",
      "        [9.9962e-01, 3.8238e-04],\n",
      "        [9.9607e-01, 3.9333e-03],\n",
      "        [9.4205e-01, 5.7950e-02],\n",
      "        [9.9866e-01, 1.3445e-03],\n",
      "        [6.1728e-01, 3.8272e-01],\n",
      "        [9.9915e-01, 8.5294e-04],\n",
      "        [9.9879e-01, 1.2052e-03],\n",
      "        [3.3582e-01, 6.6418e-01],\n",
      "        [1.5031e-05, 9.9998e-01],\n",
      "        [1.5329e-01, 8.4671e-01],\n",
      "        [2.1194e-01, 7.8806e-01],\n",
      "        [1.5107e-03, 9.9849e-01]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:04,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.7257e-03, 9.9427e-01],\n",
      "        [9.9667e-01, 3.3319e-03],\n",
      "        [9.9068e-01, 9.3216e-03],\n",
      "        [8.5154e-01, 1.4846e-01],\n",
      "        [3.1522e-01, 6.8478e-01],\n",
      "        [6.9173e-01, 3.0827e-01],\n",
      "        [1.7285e-01, 8.2715e-01],\n",
      "        [8.6064e-01, 1.3936e-01],\n",
      "        [3.3234e-02, 9.6677e-01],\n",
      "        [9.5069e-01, 4.9310e-02],\n",
      "        [9.6109e-01, 3.8910e-02],\n",
      "        [8.5959e-02, 9.1404e-01],\n",
      "        [6.2503e-01, 3.7497e-01],\n",
      "        [1.7761e-01, 8.2239e-01],\n",
      "        [4.9587e-01, 5.0413e-01],\n",
      "        [9.9989e-01, 1.0852e-04],\n",
      "        [9.3796e-01, 6.2036e-02],\n",
      "        [9.9942e-01, 5.7844e-04],\n",
      "        [4.0114e-01, 5.9886e-01],\n",
      "        [5.2926e-03, 9.9471e-01],\n",
      "        [8.8043e-01, 1.1957e-01],\n",
      "        [9.9209e-01, 7.9126e-03],\n",
      "        [4.1224e-01, 5.8776e-01],\n",
      "        [9.2052e-01, 7.9480e-02],\n",
      "        [9.6154e-01, 3.8458e-02],\n",
      "        [1.6352e-01, 8.3648e-01],\n",
      "        [9.9972e-01, 2.8482e-04],\n",
      "        [9.3031e-01, 6.9694e-02],\n",
      "        [2.2678e-03, 9.9773e-01],\n",
      "        [9.9915e-01, 8.5020e-04],\n",
      "        [9.9980e-01, 2.0003e-04],\n",
      "        [9.9832e-01, 1.6762e-03]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:06,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8.2033e-02, 9.1797e-01],\n",
      "        [1.5390e-03, 9.9846e-01],\n",
      "        [5.0252e-01, 4.9748e-01],\n",
      "        [9.9343e-01, 6.5723e-03],\n",
      "        [3.6131e-01, 6.3869e-01],\n",
      "        [9.8119e-01, 1.8815e-02],\n",
      "        [9.9897e-01, 1.0309e-03],\n",
      "        [9.9858e-01, 1.4247e-03],\n",
      "        [9.9519e-01, 4.8136e-03],\n",
      "        [9.9888e-01, 1.1216e-03],\n",
      "        [1.0708e-01, 8.9292e-01],\n",
      "        [9.9983e-01, 1.6813e-04],\n",
      "        [9.9984e-01, 1.5988e-04],\n",
      "        [9.9999e-01, 6.7867e-06],\n",
      "        [9.1940e-01, 8.0596e-02],\n",
      "        [1.6587e-01, 8.3413e-01],\n",
      "        [3.8572e-01, 6.1428e-01],\n",
      "        [9.7901e-01, 2.0991e-02],\n",
      "        [9.9004e-01, 9.9645e-03],\n",
      "        [1.9205e-01, 8.0795e-01],\n",
      "        [4.3341e-01, 5.6659e-01],\n",
      "        [6.4614e-01, 3.5386e-01],\n",
      "        [9.8880e-01, 1.1200e-02],\n",
      "        [4.4713e-01, 5.5287e-01],\n",
      "        [9.9541e-01, 4.5938e-03],\n",
      "        [7.0580e-01, 2.9420e-01],\n",
      "        [9.9934e-01, 6.5843e-04],\n",
      "        [9.9758e-01, 2.4212e-03],\n",
      "        [2.5312e-02, 9.7469e-01],\n",
      "        [1.0622e-02, 9.8938e-01],\n",
      "        [9.9998e-01, 2.0470e-05],\n",
      "        [9.9541e-01, 4.5854e-03]], device='cuda:0')\n",
      "tensor([[0.0193, 0.9807],\n",
      "        [0.0218, 0.9782],\n",
      "        [0.8715, 0.1285],\n",
      "        [0.1075, 0.8925]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(TRAINED_MODEL_PATH)['model'])\n",
    "\n",
    "# Prediction\n",
    "file_lst = []\n",
    "pred_lst = []\n",
    "prob_lst = []\n",
    "model.eval()\n",
    "example_images = []\n",
    "with torch.no_grad():\n",
    "    for batch_index, (img, file_num) in tqdm(enumerate(test_dataloader)):\n",
    "        img = img.to(DEVICE)\n",
    "        pred = model(img)\n",
    "        print(pred)\n",
    "        file_lst.extend(list(file_num))\n",
    "        pred_lst.extend(pred.argmax(dim=1).tolist())\n",
    "        prob_lst.extend(pred[:, 1].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c07bb1",
   "metadata": {},
   "source": [
    "### 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6bc2f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'file_name':file_lst, 'COVID':pred_lst})\n",
    "# df.sort_values(by=['file_name'], inplace=True)\n",
    "df.to_csv('0212_vgg11.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4eb94446-0b78-463f-9dc1-4b0a60811d61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>COVID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  file_name  COVID\n",
       "0     0.png      0\n",
       "1     1.png      0\n",
       "2     2.png      0\n",
       "3     3.png      1\n",
       "4     4.png      0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_pred = pd.read_csv('0212_vgg11.csv')\n",
    "# df_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c240ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pred = pd.read_csv('0212_vgg16.csv')\n",
    "# df_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6afa74f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>COVID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  file_name  COVID\n",
       "0     0.png      0\n",
       "1     1.png      0\n",
       "2     2.png      0\n",
       "3     3.png      1\n",
       "4     4.png      0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_pred = pd.read_csv('0212_vgg13.csv')\n",
    "# df_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e07c36-5d31-46eb-817c-a1141d96426e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
